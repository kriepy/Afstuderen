\documentclass[11pt,a4paper]{article}
\title{Behaviour Analysis of Elderly using topic models}
\author{Kristin Rieping}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}

\maketitle
%----------------------------------INTRODUCTION--------------------------------------------
\section{Introduction}
I find it a kind of hard to write down the introduction. I miss the basic motivation why we use this approach and especially the final goal of these methods. That is why I only wrote down some ideas in this part:\\



Why is activity recognition done?? What is it used for?
Monitor elderly? \\
Check if elderly people do their daily routines normally. That there are no abnormal behaviours like setting coffee 10 times in an hour. To find out if they are mentally and physical fit. To detect dangerous situations.\\

Activity recognition is hard. Finding the excact activity from sensor data can be difficult for different reasons:\\
- fuzzy timelength, it is not always clear when an activity starts or ends.\\
- activities can be done in different ways. Making koffie: first water or first filter?\\

Another problem is that many activity recognition models are based on labeled data. Labeling is timeconsuming and also effects the behaviour of the people while doing the labeling.\\

For these reasons we want to find another way to describe the behaviour of people in their house environment using an unsubervised method. We do not focus so much on the exact activities but more on the global behaviour during a day.



%--------------------------------METHOD:LDA-----------------------------------------------
\section{Topicmodel for daily behaviour of people in home enviroments}
In this section we give a global overview how we analyse the daily behaviour in peoples life. First we give an introduction of the data that is used. Then we introduce the topic model that captures the behaviours. We go in more detail about the topic model 'Latent Dirichlet Allocation' and how we use it. We also describe the problems that this approach has and how they can be solved.


\subsection{Data description}
Sensors are placed in the homes of elderly people. These sensors are located at different places in the homes. For example at doors, cubboards, microwave, toiletflush or several other places that a frequently used. Several sensors are grouped together and form a field. In the following descriptions we use five fields, which are $\{$'kitchen','livingroom','bathroom','bedroom','hallway'$\}$. The sensors that define a field can differ between different homes, they are specified manually.
From the sensors we receive the time when it is triggered and in which value it changes to. Possible values are $\{$0,1$\}$.

\subsection{Topic model for time sequential data}
To capture the daily beviour of a person we use a topic model to describe the data properly. Topic models are often used in document clustering and assumes that every document is a combination of different topics. In our case a topic can be something like 'getting up early' or 'leaving the house in the afternoon'. We do not want to capture activities like 'making coffee' or 'taking a shower', because recognizing these small or short activities are hard to detect due to variation in sensor data. This can be caused through noisy data, time differences of doing the activity or the order that the activity is done.
Therefor we only capture the activities globally.\\
A day in a persons life then can be described with a combination of all existing topics. It might be possible to group days together like weekend or week days, which have similar behaviours during the day. This approach is closly related to the approach described in [cellphone article].



\subsection{Latent dirichlet allocation with our Data}
The generative model 'Latent Dirichlet Allocation' [Blei] is a way to describe a topic model. In this model it is assumed that a Corpus can be generated from a distribution of topics. And every topic is described with a distribution of words.
In our case the Corpus is a set of days and every day is representing a document. We need to define words that can build a day. In a first approach we seperate the sequential data into timeslices of half an hour and count the amount of times that the sensors in one field are triggered. 
That means if a field contains three sensors, the amount of triggers of every sensor is add together, which gives one value for every field in one timeslice.
We do not take into account how long a sensor has the value 1 for example, but only if it is triggered or not. Therefor we have five discrete values for every timeslice.\\
Further we also want to capture the transition between the timeslices and therefor we take the values of the previous and following timeslices also into account.\\
Finally we also want to take the time during the day into account. Therefor we devide a day into 5 timezones and add this as an extra dimension of a timeslice.\\
In total we will have 16 values for every timeslice. Every value can theorectly vary between 0 and infintiy, but the maximum value we observe is ..(need to check). All 16 observations together form a word. Every day will have 48 words.\\

Generative Process building a Corpus:\\
1. Choose $\theta \sim Dir(\alpha)$.\\
2. For each of the N words $w_n$:\\
(a) Choose a topic $z_n \sim Multinomial(\theta)$.\\
(b)	Choose a word $w_n$ from $p(w_n |z_n;\beta)$, a multinomial probability conditioned on the topic
$z_n$.\\
Like described in [Blei] the model variables alpha and beta are determined with an EM-algorithm.




\subsection{Preprocessing data with k-means}


In the case of our data the size of the dictionary, which contains all unique words in the whole corpus is relative large with respect to the size of the data. There are a lot of different words, which may only differ in only one value in one of the 16 dimension that a word is made of. The LDA model does not group similar words together. For this reason the daily behaviour is not captured well.
To reduce the sice of the dictionary we first cluster the data with respect to the 16 dimensions of a word. After that we use the EM-algorithm to determine the model variables of LDA.


\subsection{Extension of LDA model}
Preprocessing the data with k-means may influence the LDA model. To capture the clusters directly in the model we adjusted the LDA model in the following way:\\
The probality matrix beta is not longer the probability of a word given a topic. But it is the mixture of gaussians of the five fields given a topic. So one topic is a combination of six gaussians, where the sixth gaussian is modelling the time value during a day and the other five gaussians model the amount of sensor triggers in a field. So the matrix beta will hold for every topic the values of six gaussian, which means 12 values (sigma and mu for every gaussian).
 So the only part that is changing in the generative process of LDA decribed above is part 2(b). The word is not choosen from a list of possible words, but it is generated with the six gaussian models.\\
 
 Note:\\
 I am not sure if in this part also the previous and following timeslice should be modelled. I think it need to be done otherwise the transition is not captured anymore.



\end{document}
