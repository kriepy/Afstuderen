

In the first section of this chapter the general idea of topic models is introduced. This section is followed by a description of how this kind of models can be applied the sensor data. After that the extension of the LDA model `LDA-Gaussian' is given, including the EM-algorithm that is used to determine the model parameters. This chapter is finalized with the description of the LDA-Poisson model.

\section{Introduction to Topic Models}
Topic models are often used in the field of document classification. Given a set of documents (corpus) it is assumed that every document is a mixture of topics. For example a news article may belongs for some percentage, let us say 30 \%,  to the topic `Economy' and for 70 \%  to the topic `Politics'. Another document of the same Corpus may belong to the topic `Economy' with 50 \%, `Politics' with 30 \% and `Finance' with 20 \%. There might be a lot of different topics and the topics can have different level of details. \\
The topics are defined by several words that can occur in the documents. The topic `Economy' may be defined by the list of words \{'trade', `industry', `GDP'\}. Other topics have different lists of words that describe them. The list might be longer or shorter and the words in the list will depend on the corpus that is used to generate the topics. It might also be the case that one word belongs to multiple topics. Eventually we can find the topic distribution of a document according to the words that are included in this document.\\
In the topic model `Latent Dirichlet Allocation' (LDA) it is assumed that a Corpus can be made out of a generative process. The parameters that generate the corpus are than used to describe the model of the corpus. The generative process which builds a corpus is as follows:\\
For every document that is generated in the Corpus
\begin{enumerate}
 \item Choose the number of words in the document from $N \sim Poisson(\xi)$.
 \item Choose a topic distribution $\theta \sim Dir(\alpha)$ for the document.
 \item For each of the N words $w_n$:
 
 \begin{enumerate}
  \item Choose a topic $z_n \sim Multinomial(\theta)$.
  \item Choose a set of words $w_n$ from the set of all words $V$ from $p(o_n |z_n;\beta)$, a multinomial probability conditioned on the topic $z_n$. Where $\beta$ is the distribution over words given a topic.
 \end{enumerate}

\end{enumerate}

The model is also shown in figure~\ref{fig:modelBasic}. The parameters $\alpha$ and $\beta$ define a corpus.

\begin{figure}[h!]
\centering
\def\svgwidth{400pt}
\input{Pictures/ModelBasic.pdf_tex}
\caption{Graphical representation of the LDA model}
\label{fig:modelBasic}
\end{figure}

To determine the model parameters an EM-algorithm can be used. How this algorithm can be applied is extensively described in~\cite{blei2003latent}. \\
In the next section it is described how this model can be used with the sensor data, that is gained in the different houses.


\section{Topic models with Sensor Data}

In order to employ the topic model with the sensor data, first the different levels of description are introduced and related to the terms of document classification.
\begin{itemize}
 \item \textbf{Dataset/Corpus}: One dataset $C$ describes the sensor data that is obtained in the home of a single person. So for every person there is a separate dataset. This set of data can be compared with a Corpus in document classification.
 \item \textbf{Day/Document}: Every dataset is divided in days, which is a natural choice because daily routines are a good indication for behavior patterns. A day can be compared with a document in a Corpus.
 \item \textbf{Observations/Words}: Finally every day is built of a set of observations, which are already introduced in chapter \ref{chapter:features}. Observations can be roughly compared with words in document classification. A different name for this observations is  `Artificial words'.
\end{itemize}

There are some differences between the data that is used for topic detection in text documents and in data obtained from binary sensors.
The main difference is that words that look similar to each other, like ``illusion" and ``allusion", may belong to a totally different topic in the document classification. But in the case of sensor data, two observation that are similar to each other, are more likely to refer to the same topic. So for example if the topic ``preparing breakfast" has the observation $o_n = {2,4,0,3,2,2}$ a similar observation like $o_n={2,4,0,4,2,2}$, where only the fourth value of the vector is changed, may also refer to the same topic. With a BOW model, where every unique observation forms a new dimension, this similarity cannot directly be captured. The only way to capture this relationship is by observing the similar observations with another observation. Say observation A and B are similar to each other. So if observation A is seen with observation C and also B is seen with C, then it might be possible that the LDA algorithm assigns the same topic for A and B. A lot of data is necessary to find these kind of relations.
Another difference is that in the topic model LDA, it is assumed that the order of the words in the text is not important. However with sensor data the time when an observation is made is of big influence. By adding the time value to the observation we can overcome this problem. The fine grained time representation (see chapter \ref{chapter:features}) will contain more information about the sequential order of the observations than the coarse grain representation.\\


% Plaatje a) omschrijven
If the Bag-of-Words model will be applied to the data, a dictionary of all unique observations must be made. All this observations then can be seen as a different dimension which are independent of each other. In figure \ref{fig:FSBOW} a) an example is shown how a topic model can assign topics to the different observations. It is not necessary that two observations lay close to each other if they are assigned to the same topics. So the spatial relationship between the observation is not taken into account. Observations can be assigned to multiple topics.\\
\begin{figure}
 \centering
 \begin{tabular}{c c}
  \includegraphics[width=0.5\textwidth]{Pictures/BOW.png} & \includegraphics[width=0.5\textwidth]{Pictures/kMeans.png}\\
  a) & b)
 \end{tabular}
 \caption{a) The Bag-of-words model with topic assignment. b) Feature space with k-means and topic assignment.} 
\label{fig:FSBOW}
\end{figure}

% plaatje b) omschrijven
To make sure that similar observations will be grouped together and appear in the same topic, the k-means algorithm can be applied to find clusters in the data. The id's of the centroids of the clusters then function as the `artificial words'. The size of the dictionary of the BOW model is in this way reduced. Applying LDA on this reduced representation can lead to a better topic description. This is shown in figure \ref{fig:FSBOW} b).\\

\begin{figure}[h]
\centering
\begin{tabular}{c c} 
\def\svgwidth{0.45\textwidth} \input{Pictures/GMM1.pdf_tex} & \def\svgwidth{0.45\textwidth} \input{Pictures/GMM2.pdf_tex}\\
a) & b)
\end{tabular}
\caption{a) GMM and LDA. b) GMM and LDA combined in one model.}
\label{fig:GMM+LDA}
\end{figure}

% plaatje c) omschrijven
The outcome of the LDA model does strongly depend on the outcome of the apriori used cluster algorithm. Choosing the correct number of clusters for the k-means algorithm can be difficult, as it is also pointed out in the work of Casale \cite{Casale:2009}. And still it can occur that the clusters do not give a good representation of the data. All clusters have the same influence on the LDA model, which is not always desirable. A Gaussian Mixture model (GMM) can give a better description of the clusters and give a degradation of the topics. Every Gaussian distribution can be assigned to a topic with the LDA model. This is shown in figure \ref{fig:GMM+LDA} a) with the red and blue Gaussian distributions. The red distribution is assigned to one topic and the two blue distributions are assigned to another topic.\\

Combining the clustering and the Topic model directly together can lead to a more global representation of the topics. This is indicated with the green Gaussian distribution in figure \ref{fig:GMM+LDA} b). 




\section{LDA-Gaussian Model}
In this section the LDA-Gaussian model is described. First the model is explained, which is followed by the description of the variational inference. This inference step is necessary to calculate the model parameters. In this subsection also the two steps of the EM-algorithm are explained.


\subsection{Model Description}
  
  For the model it is assumed that every day in a dataset can be represented as random mixture of latent topics, where every topic can be described as a distribution over observations. The following generative process is assumed for every day $m$ in the data set $C$:
\begin{enumerate}
 \item The number of observations on a day $m$ is fixed with size $N$ (for every day the same size).
 \item A day has a distribution over the topics given with $\theta \sim Dir(\alpha)$.
 \item For each of the N observations on a day $o_n$:
 
 \begin{enumerate}
  \item Estimate the topic $z_n \sim Multinomial(\theta)$.
  \item An observation $o_n$ is gained from $p(o_n |z_n,\boldsymbol\mu,\boldsymbol\sigma)$,  which is a probability that can be drawn from a set of Gaussian distributions. This probability is conditioned on the topic $z_n$ and the Gaussian Parameters $\vec{\mu_{z_n}}$ and $\vec{\sigma_{z_n}}$ of length $d$ that belong to the estimated topic $z_n$.
 \end{enumerate}

\end{enumerate}
  
In this model the number of topics $k$ is assumed to be known and fixed and with it the size of the topic variable $z$.
The probability for the observations is parametrized with two matrices $\boldsymbol\mu$ and $\boldsymbol\sigma$, both of size $D\times k$, where $D$ is the number of dimensions in an observation and $k$ the number of topics. They present the mean and standard deviation respectively and for every topic $i$ and every dimension $d$ there is a set of parameters, which describes a Gaussian distribution. Every value of a dimension for an observation $o_{ndi}$ can then be drawn from a Gaussian Distribution $\mathcal{N}(\mu_{di},\sigma_{di})$.\\
With the feature representation given in chapter \ref{chapter:features} the size of the dimension is fixed with $D=6$. $\alpha$ represents the Dirichlet parameter and is vector of length $k$.

In figure \ref{fig:modelExt} the graphical representation of the model is shown. It differs from the basic LDA model (figure \ref{fig:modelBasic}) in the description of the topic distribution $\beta$, which is here replaced with $\mu$ and $\sigma$.

\begin{figure}[h!]
\centering
\def\svgwidth{0.8\textwidth}
\input{Pictures/ModelExt.pdf_tex}
\caption{Graphical representation of the LDA-Gaussian model}
\label{fig:modelExt}
\end{figure}




  
Assuming that the generative process described above can describe the available data properly, the optimal parameters need to be found. Therefore the probability for the dataset $C$ needs to be maximized, with respect to the parameters $\alpha$, $\mu$ and $\sigma$.
This probability looks like this
\begin{equation} \label{eq:Data}
p(C|\alpha,\mu,\sigma) = \prod_{m=1}^M p(\textbf{o}_m|\alpha,\mu,\sigma)
\end{equation}
where $M$ is the number of days within the dataset.\\

For one day the joint distribution of a topic mixture $\theta$, a set of $N$ topics $\textbf{z}$ and a set of $N$ observations $\textbf{o}$ is:
\begin{equation} \label{eq:Joint}
 p(\theta,\textbf{z},\textbf{o}|\alpha,\mu,\sigma) = p(\theta|\alpha) \prod_{n=1}^N p(z_n|\theta) p(\vec{o}_n|z_n,\mu,\sigma)
\end{equation}.
Integrating over $\theta$ and summing over $z$ the marginal distribution of one document is obtained with
\begin{equation} 
p(\textbf{o}|\alpha,\mu,\sigma) = \int p(\theta|\alpha)  \left( \prod_{n=1}^N \sum_{z_n} p(z_n|\theta) p(\vec{o}_n|z_n, \mu,\sigma)  \right) d\theta
\end{equation}
This probability can then be substituted into the equation \ref{eq:Data}.\\

In the next section it is described how the optimal parameters for the model given a data set can be found.


  
\subsection{Variational Inference}
 
The marginal distribution, which is given in the previous section, can be written in terms of the parameter $\alpha$, $\mu$ and $\sigma$ as
  \begin{equation}
   p(\textbf{o}_m|\alpha,\mu,\sigma) = \frac{\Gamma (\sum_i \alpha_i)}{\prod_i \Gamma(\alpha_i)} \int \left( \prod_{i=1}^k \theta_i^{\alpha_i-1} \right)
   \left( \prod_{n=1}^N \sum_{i=1}^k \prod_{d=1}^D \theta_i \mathcal{N}(o_{nd};\mu_{id},\sigma_{id} ) \right) d\theta
  \end{equation}
Due to the coupling between $\theta$ and the Gaussian parameters $\mu$ and $\sigma$ this probability is intractable to compute.	




That is why a convexity-based variational algorithm is applied, so that the log-likelihood of a given dataset can be approximated. 
An approximation of the model is given with 
  \begin{equation}
   q(\theta,z|\gamma,\phi) = q(\theta|\gamma) \prod_{n=1}^N q(z_n|\phi_n).
  \end{equation}
In this model $\gamma$ represents the Dirichlet parameter and $\phi$ are the multinominal parameter which can be viewed as the probability $p(z_i|o_n)$ and is given as a $k \times N$-matrix for every day $m$. The graphical representation of the model is shown in figure \ref{fig:ModelApprox}.
  
 
\begin{figure}[h!]
\centering
\def\svgwidth{0.4\textwidth}
\input{Pictures/approxModel2.pdf_tex}
\caption{Approximation of the model.}
\label{fig:ModelApprox}
\end{figure}
 
  
  
  
  Given the variational distribution for the approximate model the lower bound of the log-likelihood can be estimated with the Jensen inequality
  \begin{equation}
   \begin{split}
    L(\gamma;\phi;\alpha;\mu;\sigma) =& E_q[\log p(\theta|\alpha)] + E_q[\log p(\textbf{z}|\theta)] + E_q[\log p(\textbf{o}|\textbf{z},\mu,\sigma)] \\
   & -E_q[\log p(\theta)] - E_q[\log q(\textbf{z})]
   \end{split}
  \end{equation}

In terms of the model parameters and the variational parameters this becomes
\begin{equation}
  \begin{split}
 L(\gamma;\phi;\alpha;\mu;\sigma) =& \log \Gamma (\sum_{j=1}^k \alpha_j) - \sum_{i=1}^k \log \Gamma(\alpha_i) + \sum_{i=1}^k (\alpha_i-1)(\Psi(\gamma_i)-\Psi(\sum_{j=1}^k \gamma_j)) \\
 & + \sum_{n=1}^N \sum_{i=1}^k \phi_{ni} (\Psi(\gamma_i)-\Psi(\sum_{j=1}^k \gamma_i)) \\
  & + \sum_{n=1}^N \sum_{i=1}^k \sum_{d=1}^D \phi_{ni} \log( \mathcal{N}(o_{nd};\mu_{id},\sigma_{id})) \\
  & - \log \Gamma (\sum_{j=1}^k \gamma_j) + \sum_{i=1}^k \log \Gamma (\gamma_i) - \sum_{i=1}^k (\gamma_i -1)(\Psi(\gamma_i)-\Psi(\sum_{j=1}^k \gamma_j)) \\
 & - \sum_{n=1}^N \sum_{i=1}^k \phi_{ni} \log \phi_{ni}
  \end{split}
  \label{eq:likeli}
\end{equation}
Notice that in the implementation the Gaussian distribution is integrated over a time interval of $[{o_{nd}-0.5},{o_{nd}+0.5}]$, so that the log-likelihood becomes a probability value. In this way numerical errors in the log-likelihood are smoothed.\\

An EM-process is able to maximize this lower bound on the log-likelihood. The two steps are:
  \begin{enumerate}
   \item \textbf{E-step:} For each day $m$, optimize the variational parameters $\{ \gamma_{m}*,\phi_{m}* \}$
   \item \textbf{M-step:} Maximize the resulting lower bound on the log-likelihood with respect to the model parameters $\alpha$, $\mu$ and $\sigma$.
  \end{enumerate}
  
  Now a more detailed description on both of these steps is given:
  
  \paragraph{E-step}
In the E-step of the algorithm the variational parameters $\phi$ and $\gamma$ are optimized. To get the update function for $\phi$ all terms of the lower bound of the log-likelihood in equation (\ref{eq:likeli}) are gathered, that contain the variable $\phi$. 
Take $y_i=\sum_{d=1}^D \mathcal{N}(o_{nd};\mu_{id},\sigma_{id})$ and add the constraint $\sum_{i=1}^k \phi_{ni}=1 $ . This results in the following formula
\begin{equation}
 L_{[\phi_{ni}]} = \phi_{ni}(\Psi(\gamma_i)-\Psi(\sum_{j=1}^k \gamma_j)) + \phi_{ni} \log(y_i) + \lambda_n(\sum_{j=1}^k \phi_{ni} -1)
\end{equation}
From this equation the derivative can be taken and set it to zero. This leads to the first update function
\begin{equation}
 \phi_{ni} \propto \log(y_i) \exp(\Psi(\gamma_i) - \Psi(\sum_{j=1}^k \gamma_j))
\end{equation}
\\

A similar approach is handled for $\gamma$. Again all terms of equation \ref{eq:likeli} that contain this variable is selected and the derivative is set to zero. This leads to the second update equation
\begin{equation}
 \gamma_i = \alpha_i + \sum_{n=1}^N \phi_{ni}
\end{equation}


\paragraph{M-Step}
  
In the M-step the parameters of the Gaussian distribution $\mu$ and $\sigma$ are estimated with the weighted arithmetic mean calculated over all observation in a dataset given the parameter $\phi$, which is gained in the previously e-step. This leads to the update formulas
\begin{equation}
 \mu_{di} = \frac{\sum_{m=1}^M \sum_{n=1}^N o_{dn} \phi_{ni} }{\sum_{m=1}^M \sum_{n=1}^N  \phi_{ni}}
\end{equation}
and
\begin{equation}
 \sigma_{di} = \sqrt{\frac{\sum_{m=1}^M \sum_{n=1}^N o_{dn}^2 \phi_{ni} }{\sum_{m=1}^M \sum_{n=1}^N  \phi_{ni}} - \mu_{di}^2}
\end{equation}\\

To calculate the parameter $\alpha$ again all terms of the likelihood that contains the variable $\alpha$ are selected. The derivative in the Hessian form is
\begin{equation}
 \frac{\partial L}{\partial \alpha_i\alpha_j} =  m(i,j) M \Psi'(\alpha_i) - \Psi'(\sum_{j=1}^k \alpha_j)
\end{equation}
On this equation the Newton-Rhapson method can be used to calculate the optimal $\alpha$.

\section{LDA-Poisson Model}

The feature description in chapter \ref{chapter:features} generates positive integers. Small values are more likely to occur in the observations and that is why a Poisson distribution is a better choice for the description of the topics. In figure \ref{fig:LDAPoisson} the graphical representation of LDA-Poisson model is shown. 

\begin{figure}[h!]
\centering
\def\svgwidth{0.8\textwidth}
\input{Pictures/ModelPois.pdf_tex}
\caption{Graphical representation of the LDA-Poisson model.}
\label{fig:LDAPoisson}
\end{figure}

The variational Inference and the EM-procedure will be the same as described before, except for the Gaussian distribution that is exchanged with the Poisson distribution. The parameter $\lambda$ which describes the Poisson distribution is calculated in the M-step with
\begin{equation}
 \lambda_{di} = \frac{\sum_{m=1}^M \sum_{n=1}^N o_{dn} \phi_{ni} }{\sum_{m=1}^M \sum_{n=1}^N  \phi_{ni}}.
\end{equation}

